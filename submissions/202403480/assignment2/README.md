# MNIST 분류 실험 결과

## 기본 모델 성능
- 최종 테스트 정확도: 96.81%
- 훈련 시간: 약 1분 30초

## 실험 결과
### 실험 1: [하이퍼파라미터 튜닝 (learning rate, batch size, epoch 변경)]
- 변경사항: learning rate [0.01, 0.001, 0.0001] / batch size [64, 128, 256] / epochs [3, 5, 10]
학습률, 배치 크기, 학습 횟수를 자동화 과정을 통해 조건을 다양하게 바꿔보며, 많은 조합을 통해 경향성을 살펴보았습니다. 
- 결과: 가장 높은 테스트 정확도는 97.76%로서 learning rate = 0.001, batch size = 128, epochs = 10에서 나타났습니다.
- 분석: (learning rate 관점) 1. 전반적으로 learning rate = 0.001에서 정확도가 가장 안정적이고, 높은 경향성을 보였습니다. 2. 0.01과 같은 큰 learning rate는 정확도의 변동성이 크게 나온 것을 보아 학습률을 무조건 크게 한다고 좋은 것은 아니란 생각이 듭니다. / (batch size 관점) 1. batch size가 커질수 학습 시간이 증가되는 것을 확인 할 수 있었습니다. 2. 큰 batch size일 때 epoch 값이 낮으면, 오히려 같은 조건에서의 낮은 batch size일 때보다 정확도가 낮게 나온다는 사실을 알 수 있었습니다. 즉, batch size가 커질수록 그에 따른 epoch 값도 커져야 높은 정확도가 나온다는 것을 알 수 있었습니다. / (epochs 관점) 1. epoch는 횟수를 늘릴수록 정확도가 높아지는 경향성을 보였습니다. 2. 이는 '학습 횟수가 많아질수록 모델의 성능은 당연히 좋아질 것이다'라고 예상을 한 제 예측과 동일한 결과를 보였습니다.

### 실험 2: [모델 구조 개선 (은닉층 추가, Dropout 적용)]
- 변경사항: 은닉층 구조 [256, 128] -> [512, 256, 128] / dropout 비율 [0.0, 0.5] / epochs [3, 5, 10]
- 결과: 가장 높은 테스트 정확도는 97.81%로서 은닉층 = [512, 256, 128], dropout = 0.0, epochs = 10에서 나타났습니다.
- 분석: (은닉층 관점) 1. 은닉층을 추가 한 결과 정확도가 이전보다 상승되는 것을 확인 할 수 있었습니다. 2. 은닉층을 추가하니 학습 시간이 이전보다 오래걸리는 것을 확인하였습니다. / (dropout 관점) 1. dropout = 0.5 적용시 0.0일 때보다 정확도가 낮아지는 경향성을 확인 할 수 있었습니다. 2. dropout이 학습할때 뉴런들이 특정 패턴에 과도하게 의존하지 않도록 하기 위해 학습중에 무작위로 일부 뉴런을 비활성화하는 방법이다보니, 정의의에서도 예측 할 수 있듯이 정확도는 떨어지지만 일반화는 잘 되는 것을 생각할 수 있었습니다. (훈련 정확도 감소 / 테스트 정확도 안정적)


## 결론 및 인사이트
- 가장 효과적인 개선 방법: learning rate = 0.001, batch size = 128, epochs = 10, 은닉층 [512, 256, 128], dropout = 0.0 적용 모델. (학습 횟수와 은닉층은 다다익선이다.)
- 관찰된 패턴: 1. dropout을 적용하면 정확도가 낮아지지만, 더 일반화된 결과가 나온다는 것을 알 수 있었습니다. 2. epoch가 많아질수록 정확도가 높아지는 경향이 있습니다. 3. 은닉층이 많아질수록 정확도가 높아지는 경향이 있습니다.
- 추가 개선 아이디어: 1. 활성화 함수를 ReLU 대신 다른 것을 사용해본다. 2. learning rate를 고정해서 모든 학습을 진행하는 것이 아니라, 각 epoch마다 learning rate를 다르게 해서 학습을 해본다.
